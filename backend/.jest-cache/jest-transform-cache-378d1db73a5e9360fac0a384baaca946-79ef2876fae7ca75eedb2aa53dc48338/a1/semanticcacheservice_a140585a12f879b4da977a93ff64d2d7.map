{"file":"C:\\Users\\Alejandro\\AIGestion\\backend\\src\\services\\semantic-cache.service.ts","mappings":";;;;;;;;;;;;AAAA,yDAA2D;AAC3D,yCAAuC;AACvC,0CAAoD;AACpD,qDAA2C;AAC3C,4CAAyC;AAGlC,IAAM,oBAAoB,GAA1B,MAAM,oBAAoB;IACvB,KAAK,CAAM;IACF,oBAAoB,GAAG,IAAI,CAAC;IAC5B,eAAe,GAAG,eAAe,CAAC;IAEnD;QACE,IAAI,CAAC,KAAK,GAAG,IAAI,kCAAkB,CAAC,gBAAG,CAAC,cAAc,IAAI,EAAE,CAAC,CAAC;IAChE,CAAC;IAED;;OAEG;IACH,KAAK,CAAC,YAAY,CAAC,IAAY;QAC7B,IAAI,CAAC;YACH,MAAM,KAAK,GAAG,IAAI,CAAC,KAAK,CAAC,kBAAkB,CAAC,EAAE,KAAK,EAAE,IAAI,CAAC,eAAe,EAAE,CAAC,CAAC;YAC7E,MAAM,MAAM,GAAG,MAAM,KAAK,CAAC,YAAY,CAAC,IAAI,CAAC,CAAC;YAC9C,OAAO,MAAM,CAAC,SAAS,CAAC,MAAM,CAAC;QACjC,CAAC;QAAC,OAAO,KAAK,EAAE,CAAC;YACf,eAAM,CAAC,KAAK,CAAC,KAAK,EAAE,4CAA4C,CAAC,CAAC;YAClE,OAAO,EAAE,CAAC;QACZ,CAAC;IACH,CAAC;IAED;;;;OAIG;IACH,KAAK,CAAC,WAAW,CAAC,MAAc;QAC9B,MAAM,GAAG,GAAG,kBAAkB,IAAI,CAAC,UAAU,CAAC,MAAM,CAAC,EAAE,CAAC;QAExD,sCAAsC;QACtC,MAAM,UAAU,GAAG,MAAM,IAAA,gBAAQ,EAAC,GAAG,CAAC,CAAC;QACvC,IAAI,UAAU,EAAE,CAAC;YACf,eAAM,CAAC,IAAI,CAAC,EAAE,GAAG,EAAE,EAAE,iCAAiC,CAAC,CAAC;YACxD,OAAO,UAAU,CAAC;QACpB,CAAC;QAED,sFAAsF;QACtF,OAAO,IAAI,CAAC;IACd,CAAC;IAED;;OAEG;IACH,KAAK,CAAC,WAAW,CAAC,MAAc,EAAE,QAAgB,EAAE,GAAG,GAAG,IAAI,GAAG,EAAE;QACjE,MAAM,GAAG,GAAG,kBAAkB,IAAI,CAAC,UAAU,CAAC,MAAM,CAAC,EAAE,CAAC;QACxD,MAAM,IAAA,gBAAQ,EAAC,GAAG,EAAE,QAAQ,EAAE,GAAG,CAAC,CAAC;QAEnC,sFAAsF;QACtF,6EAA6E;IAC/E,CAAC;IAEO,UAAU,CAAC,MAAc;QAC/B,4CAA4C;QAC5C,OAAO,MAAM,CAAC,IAAI,CAAC,MAAM,CAAC,IAAI,EAAE,CAAC,WAAW,EAAE,CAAC,CAAC,QAAQ,CAAC,QAAQ,CAAC,CAAC,SAAS,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;IACtF,CAAC;CACF,CAAA;AAzDY,oDAAoB;+BAApB,oBAAoB;IADhC,IAAA,sBAAU,GAAE;;GACA,oBAAoB,CAyDhC","names":[],"sources":["C:\\Users\\Alejandro\\AIGestion\\backend\\src\\services\\semantic-cache.service.ts"],"sourcesContent":["import { GoogleGenerativeAI } from '@google/generative-ai';\nimport { injectable } from 'inversify';\nimport { getCache, setCache } from '../cache/redis';\nimport { env } from '../config/env.schema';\nimport { logger } from '../utils/logger';\n\n@injectable()\nexport class SemanticCacheService {\n  private genAI: any;\n  private readonly SIMILARITY_THRESHOLD = 0.95;\n  private readonly EMBEDDING_MODEL = 'embedding-001';\n\n  constructor() {\n    this.genAI = new GoogleGenerativeAI(env.GEMINI_API_KEY || '');\n  }\n\n  /**\n   * Generates a semantic embedding for the given text.\n   */\n  async getEmbedding(text: string): Promise<number[]> {\n    try {\n      const model = this.genAI.getGenerativeModel({ model: this.EMBEDDING_MODEL });\n      const result = await model.embedContent(text);\n      return result.embedding.values;\n    } catch (error) {\n      logger.error(error, '[SemanticCache] Error generating embedding');\n      return [];\n    }\n  }\n\n  /**\n   * Tries to find a cached response for a similar prompt.\n   * Currently uses simple key-based lookup (L2) as fallback,\n   * but prepared for Vector Search.\n   */\n  async getSemantic(prompt: string): Promise<string | null> {\n    const key = `semantic_cache:${this.hashPrompt(prompt)}`;\n\n    // First try exact match in Redis (L2)\n    const exactMatch = await getCache(key);\n    if (exactMatch) {\n      logger.info({ key }, '[SemanticCache] Exact Match Hit');\n      return exactMatch;\n    }\n\n    // TODO: Implement Vector Search (L3) once RedisVL or standard RediSearch is confirmed\n    return null;\n  }\n\n  /**\n   * Caches a response semantically.\n   */\n  async setSemantic(prompt: string, response: string, ttl = 3600 * 24): Promise<void> {\n    const key = `semantic_cache:${this.hashPrompt(prompt)}`;\n    await setCache(key, response, ttl);\n\n    // In a full implementation, we would store the embedding in a Redis Vector Index here\n    // await this.storeVector(prompt, response, await this.getEmbedding(prompt));\n  }\n\n  private hashPrompt(prompt: string): string {\n    // Simple deterministic hash for prompt keys\n    return Buffer.from(prompt.trim().toLowerCase()).toString('base64').substring(0, 64);\n  }\n}\n"],"version":3}